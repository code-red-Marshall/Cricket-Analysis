{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Load the Original Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original video\n",
    "video = cv2.VideoCapture('input_video.avi')\n",
    "\n",
    "frame_width = int(video.get(3))\n",
    "frame_height = int(video.get(4))\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f\"Video loaded: {frame_width}x{frame_height}, {fps} FPS, {total_frames} frames\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection and Tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv5 model\n",
    "net = cv2.dnn.readNet(\"yolov5.weights\", \"yolov5.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "\n",
    "out_writer_object_detection = cv2.VideoWriter('output_object_detection.avi', \n",
    "                                               cv2.VideoWriter_fourcc(*'XVID'), fps, \n",
    "                                               (frame_width, frame_height))\n",
    "\n",
    "\n",
    "with tqdm(total=total_frames, desc=\"Object Detection and Tracking\", unit=\"frame\") as pbar:\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        \n",
    "        blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward(output_layers)\n",
    "\n",
    "        boxes, confidences, class_ids = [], [], []\n",
    "        for out in detections:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5:\n",
    "                    center_x = int(detection[0] * frame_width)\n",
    "                    center_y = int(detection[1] * frame_height)\n",
    "                    w = int(detection[2] * frame_width)\n",
    "                    h = int(detection[3] * frame_height)\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "        for i in range(len(boxes)):\n",
    "            if i in indexes:\n",
    "                x, y, w, h = boxes[i]\n",
    "                label = str(classes[class_ids[i]])\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "       \n",
    "        out_writer_object_detection.write(frame)\n",
    "        pbar.update(1)\n",
    "\n",
    "video.release()\n",
    "out_writer_object_detection.release()\n",
    "print(\"Object detection and tracking video saved as 'output_object_detection.avi'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "video = cv2.VideoCapture('input_video.avi')\n",
    "\n",
    "\n",
    "backgroundObject = cv2.createBackgroundSubtractorMOG2(detectShadows=True)\n",
    "\n",
    "\n",
    "out_writer_bg_subtracted = cv2.VideoWriter('output_bg_subtracted.avi', \n",
    "                                           cv2.VideoWriter_fourcc(*'XVID'), fps, \n",
    "                                           (frame_width, frame_height))\n",
    "\n",
    "\n",
    "with tqdm(total=total_frames, desc=\"Background Subtraction\", unit=\"frame\") as pbar:\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "\n",
    "        fgmask = backgroundObject.apply(frame)\n",
    "        _, fgmask = cv2.threshold(fgmask, 250, 255, cv2.THRESH_BINARY)\n",
    "        fgmask = cv2.erode(fgmask, None, iterations=1)\n",
    "        fgmask = cv2.dilate(fgmask, None, iterations=2)\n",
    "\n",
    "\n",
    "        foregroundPart = cv2.bitwise_and(frame, frame, mask=fgmask)\n",
    "\n",
    "\n",
    "        out_writer_bg_subtracted.write(foregroundPart)\n",
    "        pbar.update(1)\n",
    "\n",
    "video.release()\n",
    "out_writer_bg_subtracted.release()\n",
    "print(\"Background-subtracted video saved as 'output_bg_subtracted.avi'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Video: Background Subtraction with Object Detection and Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "video = cv2.VideoCapture('output_bg_subtracted.avi')\n",
    "\n",
    "out_writer_combined = cv2.VideoWriter('output_combined.avi', \n",
    "                                      cv2.VideoWriter_fourcc(*'XVID'), fps, \n",
    "                                      (frame_width, frame_height))\n",
    "\n",
    "\n",
    "prev_bat_pos = None\n",
    "bat_speeds = []\n",
    "pixels_to_meters = \t0.0002645833  # Average size of a pixel\n",
    "conversion_factor = pixels_to_meters * 3600 / 1000\n",
    "speed_threshold = 10  \n",
    "\n",
    "def calculate_speed(prev_pos, curr_pos, fps):\n",
    "    distance_px = np.linalg.norm(np.array(curr_pos) - np.array(prev_pos))\n",
    "    speed_px_s = distance_px * fps\n",
    "    speed_kmh = speed_px_s * conversion_factor\n",
    "    return speed_kmh\n",
    "\n",
    "with tqdm(total=total_frames, desc=\"Combining Background Subtraction with Object Detection\", unit=\"frame\") as pbar:\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward(output_layers)\n",
    "\n",
    "        boxes, confidences, class_ids = [], [], []\n",
    "        curr_bat_pos = None\n",
    "\n",
    "        for out in detections:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5:\n",
    "                    center_x = int(detection[0] * frame_width)\n",
    "                    center_y = int(detection[1] * frame_height)\n",
    "                    w = int(detection[2] * frame_width)\n",
    "                    h = int(detection[3] * frame_height)\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "        for i in range(len(boxes)):\n",
    "            if i in indexes:\n",
    "                x, y, w, h = boxes[i]\n",
    "                label = str(classes[class_ids[i]])\n",
    "                if label == 'baseball bat':  # Replace with relevant label\n",
    "                    curr_bat_pos = (x + w // 2, y + h // 2)\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        if curr_bat_pos and prev_bat_pos:\n",
    "            curr_speed_kmh = calculate_speed(prev_bat_pos, curr_bat_pos, fps)\n",
    "            if curr_speed_kmh > speed_threshold:\n",
    "                bat_speeds.append(curr_speed_kmh)\n",
    "                cv2.putText(frame, f'Speed: {curr_speed_kmh:.2f} km/h', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        prev_bat_pos = curr_bat_pos\n",
    "\n",
    "\n",
    "        out_writer_combined.write(frame)\n",
    "        pbar.update(1)\n",
    "\n",
    "video.release()\n",
    "out_writer_combined.release()\n",
    "print(\"Combined video saved as 'output_combined.avi'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
